<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Likelihood in Diffusion Models | Rafał Karczewski </title> <meta name="author" content="Rafał Karczewski"> <meta name="description" content="A summary of our two recent articles on estimating and controling likelihood in diffusion models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafalkarczewski.github.io/blog/2025/diffusion-density-draft/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding Likelihood in Diffusion Models",
            "description": "A summary of our two recent articles on estimating and controling likelihood in diffusion models.",
            "published": "February 11, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rafał</span> Karczewski </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Likelihood in Diffusion Models</h1> <p>A summary of our two recent articles on estimating and controling likelihood in diffusion models.</p> </d-title> <d-article> <h2 id="diffusion-models-recap">Diffusion models recap</h2> <p>The idea of diffusion models <d-cite key="sohl2015deep,ho2020denoising,song2021scorebased"></d-cite> is to gradually transform the data distribution \(p_0\) into pure noise \(p_T\) (e.g. \(\mathcal{N}(0, I)\)). This is achieved via the forward noising kernel \(p_t(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\alpha_t \mathbf{x}_0, \sigma_t^2 I),\) with \(\alpha, \sigma\) chosen so that all the information is lost at \(t=T\), i.e. \(p_T(\mathbf{x}_T \mid \mathbf{x}_0) \approx p_T(\mathbf{x}_T) = \mathcal{N}(\mathbf{0}, \sigma_T^2 I).\) Hence, as \(t\) increases, \(\mathbf{x}_t\) becomes more “noisy,” and at \(t=T\) we reach a tractable distribution \(p_T\).</p> <p>This process can equivalently be written as a Stochastic Differential Equation (SDE):</p> \[d\mathbf{x}_t = f(t)\mathbf{x}_t\, dt + g(t)\,d W_t,\] <p>where \(f, g\) are scalar functions and \(W\) is the Wiener process. Remarkably, this process is reversible! The Reverse SDE <d-cite key="anderson1982reverse"></d-cite> is given by</p> \[\begin{equation}\label{eq:rev-sde} d\mathbf{x}_t = \bigl(f(t)\mathbf{x}_t - g^2(t)\nabla \log p_t(\mathbf{x}_t)\bigr)\,dt + g(t)\,d \overline{W}_t, \end{equation}\] <p>where \(\overline{W}\) is the Wiener process going backwards in time and \(\nabla \log p_t(\mathbf{x}_t)\) is the <em>score function</em>, which can be accurately approximated with a neural network <d-cite key="hyvarinen2005estimation,vincent2011connection,song2020sliced"></d-cite>. Since \(p_T\) is a tractable distribution, we can easily sample \(\mathbf{x}_T \sim p_T\) and solve \eqref{eq:rev-sde} to generate a sample \(\mathbf{x}_0 \sim p_0\).</p> <p>Rather surprisingly, there exists an equivalent <em>deterministic</em> process <d-cite key="song2021scorebased,song2020denoising"></d-cite> given by an Ordinary Differential Equation (ODE):</p> \[\begin{equation}\label{eq:pf-ode} d\mathbf{x}_t = \Bigl(f(t)\mathbf{x}_t - \frac{1}{2}g^2(t)\nabla \log p_t(\mathbf{x}_t)\Bigr)\, dt, \end{equation}\] <p>which is also guaranteed to generate a sample \(\mathbf{x}_0 \sim p_0\) whenever \(\mathbf{x}_T \sim p_T\).</p> <hr> <h2 id="what-is-log-density">What is Log-Density?</h2> <p>Diffusion models are likelihood-based models <d-cite key="song2021maximum,kingma2024understanding"></d-cite>, aiming to assign high likelihood to training data and, by extension, lower likelihood to out-of-distribution (OOD) data. Intuitively, one might think that log-density is a reliable measure of whether a sample is “in” or “out” of the data distribution.</p> <p>However, prior research <d-cite key="choi2018waic,nalisnick2018deep,nalisnick2019detecting,ben2024d"></d-cite> has shown that generative models can sometimes assign higher likelihoods to OOD data than to in-distribution data. In <d-cite key="karczewski2025diffusion"></d-cite>, we show that diffusion models are no different. In fact, we push this analysis further by exploring the <em>highest-density</em> regions of diffusion models.</p> <p>Using a theoretical <strong>mode-tracking ODE</strong>, we investigate the regions of the data space where the model assigns the highest likelihood. Surprisingly, these regions are occupied by cartoon-like drawings or blurry images—patterns absent from the training data. Additionally, we observe a strong correlation between negative log-density and PNG image size, revealing that negative log-likelihood for image data essentially measures <strong>information content</strong> (or <strong>detail</strong>), rather than “in-distribution-ness.”</p> <p>These surprising observations underscore the difference between maximum-density points and <em>typical</em> sets, which we explore next.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/cats_logp.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Likelihood measures the amount of detail in an image. Sample generated with a StableDiffusion v2.1 <d-cite key="rombach2021highresolution"></d-cite> using the High-Density sampler from <d-cite key="karczewski2025diffusion"></d-cite>. </figcaption> </div> <h3 id="why-does-this-happen">Why Does This Happen?</h3> <p>The observation that blurry or cartoon-like images have the highest density may seem counterintuitive. Does it imply the model considers these images to be the “most likely”? To clarify this, we must distinguish between <strong>probability density</strong> and <strong>probability</strong>.</p> <h4 id="probability-density-vs-probability">Probability Density vs. Probability</h4> <p>The probability of being in a region \(A\) is given by the integral of the density over that region:</p> \[P(A) = \int_A p(\mathbf{x})\, d\mathbf{x}.\] <p>If the density \(p(\mathbf{x})\) is constant in \(A\), then</p> \[P(A) = \text{(constant density)} \times \text{Vol}(A).\] <p>Hence, <em>both</em> density and volume determine the probability.</p> <h4 id="a-gaussian-example">A Gaussian Example</h4> <p>A helpful analogy is the standard normal Gaussian in high dimensions. Its density is proportional to \(\exp(-\|\mathbf{x}\|^2/2)\), which is highest at the origin \(\mathbf{x}=\mathbf{0}\). However, typical samples in high dimensions lie <em>away</em> from the origin.</p> <p>Specifically, consider a thin spherical shell of radius \(r\) and thickness \(dr\). The volume is proportional to \(r^{D-1}dr\), and the probability is</p> \[P(\text{shell at }r) \;\propto\; r^{D-1}\,\exp\Bigl(-\frac{r^2}{2}\Bigr)\,dr.\] <p>This is maximized at \(r=\sqrt{D-1}\). <d-footnote>This is because \(f(r)= r^{D-1} \exp(-r^2/2)\) satisfies \(f'(r)= r^{D-2} \exp(-r^2/2) (D-1 - r^2)\), which is zero at \(r=\sqrt{D-1}\). </d-footnote> Thus, although density is highest at the origin, that point has negligible <em>volume</em> and is not in the typical region.</p> <h4 id="diffusion-models-high-density-blurry-images-vs-high-volume-detailed-images">Diffusion Models: High-Density Blurry Images vs. High-Volume Detailed Images</h4> <p>By analogy, blurry/cartoon-like images correspond to small-volume, high-density regions, while detailed images occupy a larger-volume region with lower density. Hence, more realistic images can have <em>lower</em> log-density than simpler, less-detailed ones.</p> <hr> <h2 id="how-to-estimate-log-density">How to Estimate Log-Density?</h2> <p>To measure log-density of diffusion samples, note that sampling can vary along two dimensions:</p> <ol> <li> <strong>Deterministic vs. Stochastic Sampling</strong> <ul> <li>Deterministic: smooth trajectories (ODE-based).</li> <li>Stochastic: noisy trajectories (SDE-based).</li> </ul> </li> <li> <strong>Original Dynamics vs. Modified Dynamics</strong> <ul> <li> <em>Original Dynamics</em>: The reverse process given by \eqref{eq:rev-sde} or \eqref{eq:pf-ode}.</li> <li> <em>Any Dynamics</em>: Modified drift/diffusion to target a specific objective (e.g., controlling log-density).</li> </ul> </li> </ol> <p>We can summarize:</p> <table> <thead> <tr> <th>Sampling Mode</th> <th>Original Dynamics</th> <th>Any Dynamics</th> </tr> </thead> <tbody> <tr> <td><strong>Deterministic</strong></td> <td>Prior work <d-cite key="chen2018neural"></d-cite> </td> <td>Ours <d-cite key="karczewski2025diffusion"></d-cite> </td> </tr> <tr> <td><strong>Stochastic</strong></td> <td>Ours <d-cite key="karczewski2025diffusion"></d-cite> </td> <td>Ours <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> </td> </tr> </tbody> </table> <p>Previously, log-density was only measurable for <em>deterministic</em> sampling under the <em>original</em> dynamics. In <d-cite key="karczewski2025diffusion"></d-cite>, we extend this to deterministic sampling with modified dynamics, as well as stochastic sampling under original dynamics.<d-footnote>Once the *true* score is replaced by an approximate one, log-density estimates become biased. We derive the exact bias in <d-cite key="karczewski2025diffusion"></d-cite> and show it vanishes as the approximation error goes to zero.</d-footnote></p> <p>In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we further generalize to <em>stochastic sampling with modified dynamics</em>, deriving the log-density evolution via <strong>Itô’s Lemma</strong> and the <strong>Fokker–Planck equation</strong>. In practice, these formulas are most relevant to diffusion models because we already have (an approximation of) \(\nabla \log p_t(\mathbf{x})\). The same framework can be used for <em>any</em> continuous-time flow model, provided the score is known.</p> <hr> <h2 id="how-to-control-log-density">How to Control Log-Density?</h2> <p>An interesting empirical observation <d-cite key="song2021scorebased"></d-cite> is that simply <strong>rescaling</strong> the latent code (e.g., scaling the initial noise at \(t=T\)) can change the level of detail in the generated image. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we offer a theoretical explanation using the concept of <strong>Score Alignment</strong>, which relates scaling the latent code to changes in log-density.</p> <h3 id="score-alignment">Score Alignment</h3> <p><strong>Score Alignment (SA)</strong> measures the angle between</p> <ul> <li>the score function at \(t=T\) (noise distribution), <em>pushed forward</em> to \(t=0\) via \eqref{eq:pf-ode}, and</li> <li>the score function at \(t=0\) (data distribution).</li> </ul> <p>If this angle is always <em>acute</em>, increasing \(\log p_T(\mathbf{x}_T)\) (e.g., by scaling \(\mathbf{x}_T\)) <em>monotonically</em> increases \(\log p_0(\mathbf{x}_0)\).<d-footnote>If it is always obtuse, increasing \( \log p_T(\mathbf{x}_T)\) decreases \( \log p_0(\mathbf{x}_0)\). </d-footnote> Surprisingly, one can verify this alignment <em>without</em> fully knowing the score function; see <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> for details.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/sa_vis.png"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Score Alignment guarantees a monotonic relationship between scaling the latent code at \(t=T\) and the resulting log-density at \(t=0\). Empirically, it nearly always holds for diffusion models on image data. </figcaption> </div> <p><strong>Take-home:</strong> <em>If SA holds, simply rescaling the latent noise (\mathbf{x}_T) is a quick way to increase or decrease the final log-density (and thus control image detail).</em></p> <hr> <h3 id="density-guidance-a-principled-approach-to-controlling-log-density">Density Guidance: A Principled Approach to Controlling Log-Density</h3> <p>While latent code scaling works, it is fairly coarse. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we propose <strong>Density Guidance</strong>, a more precise way to steer how \(\log p_t(\mathbf{x}_t)\) evolves during sampling. We start from a general flow model</p> \[d\mathbf{x}_t = \mathbf{u}_t(\mathbf{x}_t)\,dt,\] <p>and want to enforce</p> \[\begin{equation}\label{eq:logp-b} \frac{d}{dt}\,\log p_t(\mathbf{x}_t) = b_t(\mathbf{x}_t), \end{equation}\] <p>for a user-defined function \(b_t(\mathbf{x}_t)\). As long as we know the score \(\nabla \log p_t(\mathbf{x})\), we can modify \(\mathbf{u}_t(\mathbf{x})\):</p> \[\begin{equation} \tilde{\mathbf{u}}_t(\mathbf{x}) = \mathbf{u}_t(\mathbf{x}) + \underbrace{\frac{\operatorname{div}\,\mathbf{u}_t(\mathbf{x}) + b_t(\mathbf{x})}{\|\nabla \log p_t(\mathbf{x})\|^2}\,\nabla \log p_t(\mathbf{x})}_{\text{log-density correction}}. \end{equation}\] <h4 id="choosing-dynamics">Choosing Dynamics</h4> <p>In diffusion models, we typically let \(\mathbf{u}_t(\mathbf{x})\) be given by the PF-ODE \eqref{eq:pf-ode}, so the original flow is</p> \[\mathbf{u}_t(\mathbf{x}) = f(t)\,\mathbf{x} - \tfrac{1}{2}g^2(t)\,\nabla \log p_t(\mathbf{x}).\] <p>If \(b_t(\mathbf{x})\) is too large, samples may leave the typical region; if it is too small, we might not observe a noticeable density shift. A practical strategy is to choose \(b_t(\mathbf{x})\) based on typical fluctuations of \(\log p_t(\mathbf{x})\) in high dimensions (e.g., the \(\mathcal{N}(0,1)\)-like behavior of certain score-based terms). One concrete example is:</p> \[\begin{equation}\label{eq:b-quantile} b^q_t(\mathbf{x}) = -\operatorname{div}\,\mathbf{u}_t(\mathbf{x}) \;-\; \frac{1}{2}\,g^2(t)\,\frac{\sqrt{2D}}{\sigma_t^2}\,\Phi^{-1}(q), \end{equation}\end{equation}\] <p>leading to a <strong>Density Guidance ODE</strong>:</p> \[\mathbf{u}_t^{\text{DG-ODE}}(\mathbf{x}) = f(t)\,\mathbf{x} - \tfrac{1}{2}g^2(t)\,\eta_t(\mathbf{x})\,\nabla \log p_t(\mathbf{x}),\] <p>where \(\eta_t(\mathbf{x})\) adaptively rescales the score.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/deterministic-steering.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> **Density Guidance** in a deterministic setting. Using \eqref{eq:b-quantile} with different \(q\) yields varying amounts of detail in samples from a pretrained EDM2 model <d-cite key="karras2024analyzing"></d-cite>. </figcaption> </div> <p><strong>Take-home:</strong> <em>Deterministic Density Guidance modifies the PF-ODE with a <strong>rescaled</strong> score, achieving fine-grained control of log-density over the entire sampling trajectory.</em></p> <hr> <h3 id="stochastic-sampling-with-density-guidance">Stochastic Sampling with Density Guidance</h3> <p>Many applications benefit from injecting noise for diversity but still require log-density control. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we introduce a stochastic variant:</p> \[\begin{equation}\label{eq:stochastic-steering} d \mathbf{x}_t = \mathbf{u}_t^{\text{DG-SDE}}(\mathbf{x}_t)\,dt + \varphi(t)\,P_t(\mathbf{x}_t)\,d\overline{W}_t, \end{equation}\end{equation}\] <p>with</p> \[\mathbf{u}_t^{\text{DG-SDE}}(\mathbf{x}) = \mathbf{u}_t^{\text{DG-ODE}}(\mathbf{x}) + \underbrace{\frac{1}{2}\,\varphi^2(t)\,\frac{\Delta \log p_t(\mathbf{x})}{\|\nabla \log p_t(\mathbf{x})\|^2}\,\nabla \log p_t(\mathbf{x})}_{\text{correction for added stochasticity}},\] <p>and</p> \[P_t(\mathbf{x}) = I - \frac{\nabla \log p_t(\mathbf{x})}{\|\nabla \log p_t(\mathbf{x})\|}\,\Bigl(\frac{\nabla \log p_t(\mathbf{x})}{\|\nabla \log p_t(\mathbf{x})\|}\Bigr)^T.\] <p>The matrix \(P_t(\mathbf{x})\) projects the Wiener increment onto the subspace orthogonal to the score. This ensures \(\log p_t(\mathbf{x}_t)\) evolves just like in the deterministic case, <em>even though</em> \(\mathbf{x}_t\) itself follows a stochastic path.</p> <d-footnote> Strictly, projecting out the score direction also introduces a small extra drift term. Empirically, this term is negligible, so we omit it in experiments. See <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> for details. </d-footnote> <p>In practice, we let \(\varphi(t)=\widetilde{\varphi}(t)\,g(t)\), controlling how much extra noise we add relative to the original SDE’s diffusion coefficient \(g(t)\). This yields a <strong>Stochastic Density Guidance</strong> method that smoothly steers \(\log p_t(\mathbf{x}_t)\) while permitting randomness in orthogonal directions. For example:</p> <ul> <li>Early noise injection changes large-scale shapes or features.</li> <li>Late noise injection changes only fine textures.</li> </ul> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/stochastic-steering.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> **Stochastic Density Guidance** adds noise without forfeiting log-density control. Samples from a pretrained EDM2 model <d-cite key="karras2024analyzing"></d-cite>, varying \(q\) and the timing of noise injection. </figcaption> </div> <p><strong>Take-home:</strong> <em>Stochastic Density Guidance = same <strong>rescaled</strong> score approach, plus a projected noise term that preserves the intended log-density schedule.</em></p> <hr> <h2 id="conclusion">Conclusion</h2> <p>Log-density is central to understanding and controlling diffusion models. Rather than signifying “in-distribution” membership, it mostly reflects the <em>amount of detail</em> in generated images. In <d-cite key="karczewski2025diffusion"></d-cite>, we investigate the peculiar high-density regions of diffusion models, revealing surprising artifacts and offering new ways to measure log-density in diverse sampling regimes. Building on this, <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> introduces <strong>Density Guidance</strong>, a framework—both deterministic and stochastic—that lets us precisely sculpt how \(\log p_t(\mathbf{x}_t)\) evolves during sampling.</p> <p>These findings deepen our theoretical grasp of diffusion models and open up <strong>practical avenues</strong> for generating images with fine-grained control over detail and variability.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Rafał Karczewski. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?c8cdae9495e998e46b0dddc10276be62"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>