<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Likelihood in Diffusion Models | Rafał Karczewski </title> <meta name="author" content="Rafał Karczewski"> <meta name="description" content="A summary of our two recent articles on estimating and controlling likelihood in diffusion models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rafalkarczewski.github.io/blog/2025/diffusion-density/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding Likelihood in Diffusion Models",
            "description": "A summary of our two recent articles on estimating and controlling likelihood in diffusion models.",
            "published": "February 11, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rafał</span> Karczewski </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Likelihood in Diffusion Models</h1> <p>A summary of our two recent articles on estimating and controlling likelihood in diffusion models.</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction"> Introduction </a></div> <div><a href="#diffusion-models-recap"> Diffusion models recap </a></div> <div><a href="#what-is-log-density">What is Log-Density?</a></div> <ul> <li><a href="#why-does-this-happen">Why Does This Happen?</a></li> </ul> <div><a href="#how-to-esimate-log-density">How to estimate Log-Density?</a></div> <div><a href="#how-to-control-log-density">How to Control Log-Density?</a></div> <ul> <li><a href="#density-guidance-a-principled-approach-to-controlling-log-density">Density Guidance: A Principled Approach to Controlling Log-Density</a></li> <li><a href="#stochastic-sampling-with-density-guidance">Stochastic Sampling with Density Guidance</a></li> <li><a href="#score-alignment">Score Alignment</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In this post, we dive into insights from our two recent works on diffusion models. In the first one <d-cite key="karczewski2025diffusion"></d-cite>, we introduce novel tools for estimating likelihood (or log-density) in diffusion models. A key finding is that high-density regions of image diffusion models tend to correspond to images with less detail—blurry textures and even cartoon-like representations. We further demonstrate that (negative) \(\log p_t\) can be interpreted as a measure of image detail. In the second work <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we take this idea further, showing that \(\log p_t\)​ can actually be controlled in both deterministic and stochastic formulations. This allows us to modulate the level of detail in images generated by diffusion models, opening up new possibilities for fine-grained control in generative modeling.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/blog-post-fig-1-labeled.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Density guidance <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> allows for controling image detail.</figcaption> </div> <h2 id="diffusion-models-recap">Diffusion models recap</h2> <p>The idea of diffusion models <d-cite key="sohl2015deep,ho2020denoising,song2021scorebased"></d-cite> is to gradually transform the data distribution \(p_0\) into pure noise \(p_T\) (e.g. \(\mathcal{N}(0, I)\)). This is achieved via the forward noising kernel \(p_t(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\alpha_t \mathbf{x}_0, \sigma_t^2 I)\) with \(\alpha, \sigma\) chosen so that all the information is lost at \(t=T\), i.e. \(p_T(\mathbf{x}_T \mid \mathbf{x}_0) \approx p_T(\mathbf{x}_T) = \mathcal{N}(\mathbf{0}, \sigma_T^2 I)\). Hence, as \(t\) increases, \(\mathbf{x}_t\)​ becomes more `noisy’, and at \(t=T\) we reach a tractable distribution \(p_T\). This process can equivalently be written as a Stochastic Differential Equation (SDE):</p> <p>\begin{equation} d\mathbf{x}_t = f(t)\mathbf{x}_t dt + g(t)d W_t \qquad \mathbf{x}_0 \sim p_0, \end{equation}</p> <p>where \(f, g\) are scalar functions and \(W\) is the Wiener process. Remarkably, this process is reversible! The Reverse SDE <d-cite key="anderson1982reverse"></d-cite> is given by</p> <p>\begin{equation}\label{eq:rev-sde} d\mathbf{x}_t = (f(t)\mathbf{x}_t - g^2(t)\nabla \log p_t(\mathbf{x}_t)) dt + g(t)d \overline{W}_t \qquad \mathbf{x}_T \sim p_T, \end{equation}</p> <p>where \(\overline{W}\) is the Wiener process going backwards in time and \(\nabla \log p_t(\mathbf{x}_t)\) is the <em>score function</em>, which can be accurately approximated with a neural network <d-cite key="hyvarinen2005estimation,vincent2011connection,song2020sliced"></d-cite>. Since \(p_T\) is a tractable distribution, we can easily sample \(\mathbf{x}_T \sim p_T\) and solve \eqref{eq:rev-sde} to generate a sample \(\mathbf{x}_0 \sim p_0\).</p> <p>Rather surprisingly, it turns out that there exists an equivalent <em>deterministic</em> process <d-cite key="song2021scorebased,song2020denoising"></d-cite> given by a Probability-Flow Ordinary Differential Equation (PF-ODE):</p> <p>\begin{equation}\label{eq:pf-ode} d\mathbf{x}_t = (f(t)\mathbf{x}_t - \frac{1}{2}g^2(t)\nabla \log p_t(\mathbf{x}_t)) dt \qquad \mathbf{x}_T \sim p_T, \end{equation}</p> <p>which is also guaranteed to generate a sample \(\mathbf{x}_0 \sim p_0\).</p> <h3 id="score-matching">Score matching</h3> <p>Diffusion models are typically trained using a score matching objective, which seeks to approximate the score function \(\nabla \log p_t(\mathbf{x})\) using a neural network \(\mathbf{s}_\theta(\mathbf{x})\):</p> \[\begin{equation}\label{eq:sm-obj} \mathbb{E}_{t, \mathbf{x}_0, \mathbf{x}_t} \Big[ \lambda(t) \|\mathbf{s}_\theta(\mathbf{x}_t, t) - \nabla \log p_t(\mathbf{x}_t)\|^2 \Big], \end{equation}\] <p>where \(\lambda(t)\) is a weighting function. Interestingly, with an appropriate choice of \(\lambda(t)=g^2(t)\), score matching corresponds to maximum likelihood training <d-cite key="song2021maximum,kingma2021variational"></d-cite>. In other words, diffusion models are likelihood-based models, as they implicitly maximize the data log-likelihood \(\log p_0 (x_0)\).</p> <h2 id="what-is-log-density">What is Log-Density?</h2> <p>Since diffusion models are likelihood-based models, they aim to assign high likelihood to training data and, by extension, low likelihood to out-of-distribution (OOD) data. Intuitively, one might think that log-density is a reliable measure of whether a sample lies in or out of the data distribution.</p> <p>However, prior research <d-cite key="choi2018waic,nalisnick2018deep,nalisnick2019detecting,ben2024d"></d-cite> has shown that generative models can sometimes assign higher likelihoods to OOD data than to in-distribution data. In <d-cite key="karczewski2025diffusion"></d-cite>, we show that diffusion models are no different. In fact, we push this analysis further by exploring the highest-density regions of diffusion models.</p> <p>We derive a theoretical <strong>mode-tracking ODE</strong> and its efficient approximation - the <em>high-density</em> (HD-) sampler, which allows for exploring the regions of high density. Specifically, it is an ODE, which converges to the approximate mode of \(p_t(\mathbf{x}_0 \mid \mathbf{x}_t)\) for any starting point \(\mathbf{x}_t\).</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/video/hd_sampling.gif"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> High-Density ODE sampler on a 1D Gaussian Mixture data starting at different values of \(t\).</figcaption> </div> <p>We use the HD-sampler on image diffusion models to investigate the highest-likelihood regions. Surprisingly, these are occupied by cartoon-like drawings or blurry images—patterns that are absent from the training data. Additionally, we observe a strong correlation between negative log-density and PNG image size, revealing that negative log-likelihood for image data is essentially a measure of <strong>information content</strong> or <strong>detail</strong>, rather than “in-distribution-ness”.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/cats_logp.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Likelihood measures the amount of detail in an image. Sample generated with a StableDiffusion v2.1 <d-cite key="rombach2021highresolution"></d-cite> using the High-Density sampler <d-cite key="karczewski2025diffusion"></d-cite>. </figcaption> </div> <h3 id="why-does-this-happen">Why Does This Happen?</h3> <p>While we don’t have a full understanding of why the blurry and cartoon-like images occupy the highest-density regions, we discuss several factors that might contribute to this phenomenon.</p> <h4 id="models-loss-function-and-out-of-distribution-freedom">Model’s Loss Function and Out-of-Distribution Freedom</h4> <p>Diffusion models are trained with variants of the Score-Matching objective \eqref{eq:sm-obj}, meaning that their loss is optimized only for in-distribution images. However, the model defines the density everywhere, including points outside the distribution which are never seen during training. This gives the model freedom in how it assigns the likelihood to <em>atypical</em> samples such as blurry or cartoon-like ones.</p> <h4 id="information-theory-and-compressibility">Information Theory and Compressibility</h4> <p>Information theory suggests that high-likelihood images should be more compressible, which translates to low-detail in the context of images. While the exact role of information theory in this context remains unclear, it aligns well with what we observe: the highest-likelihood images are low-detail, simple, and lacking in complex textures, which makes them look like cartoons or appear blurry.</p> <h4 id="the-density-trade-off-in-the-typical-set">The Density Trade-Off in the Typical Set</h4> <p>The set of realistic images is vast. Some are high-detail, while others are low-detail, but the total number of high-detail images is significantly greater due to their higher number of degrees of freedom. Since probability density must integrate to 1, this forces the model to assign lower likelihood to high-detail images simply because there are so many of them. Consider images of: apple on a plain background versus a tree with countless variations of leaves and grass: both are realistic, but the latter has vastly more possible instances, necessitating a lower individual density per image.</p> <h4 id="the-branching-tree-hypothesis">The Branching Tree Hypothesis</h4> <p>Recent work <d-cite key="karras2024guiding"></d-cite> suggests that diffusion models learn distributions in a branching tree structure. We hypothesize that at the roots of these trees—where the most likely samples exist—lie the simplest, most compressed forms of images, which manifest as blurry or cartoon-like.</p> <h4 id="high-dimensional-distributions-and-mode-paradoxes">High-Dimensional Distributions and Mode “Paradoxes”</h4> <p>Finally, it is important to recognize that the fact that the highest-density points lie outside the typical set is not unusual in high-dimensional probability distributions. A classic example is the standard \(D\)-dimensional Gaussian: its mode is at the origin, but as dimensionality increases, almost all samples are concentrated on a thin spherical shell at radius \(\sqrt{D}\). Sampling close to the mode (zero vector) has an exponentially vanishing probability as \(D\) grows <d-cite key="nalisnick2019detecting"></d-cite>. Similarly, in diffusion models, the most frequently sampled (realistic) images form a high-dimensional structure away from the peak density points.</p> <p>Now that we have a better understanding of what density means in diffusion models, we will now discuss how it is actually estimated in practice.</p> <h2 id="how-to-estimate-log-density">How to Estimate Log-Density?</h2> <p>Estimating the log-density of a generated sample \(\mathbf{x}_0\) boils down to tracking how the marginal log-densities changed over the sampling trajectory:</p> \[\log p_0(\mathbf{x}_0) = \log p_T (\mathbf{x}_T) - \left( \log p_T(\mathbf{x}_T) - \log p_0(\mathbf{x_0}) \right) = \underbrace{\log p_T(\mathbf{x}_T)}_{\text{known analytically}} + \int_T^0 \underbrace{d\log p_t(\mathbf{x}_t)}_{\text{log-density change}}\] <p>and the log-density change \(d \log p_t(\mathbf{x}_t)\) depends on how we sample. Sampling in diffusion models can be categorized along two main axes:</p> <h3 id="1-deterministic-vs-stochastic-sampling"><strong>1. Deterministic vs Stochastic Sampling</strong></h3> <p>Diffusion models generate samples by reversing the forward noising process, which can be done in one of two ways:</p> <ul> <li> <strong>Deterministic sampling:</strong> Uses smooth, noise-free trajectories of PF-ODE \eqref{eq:pf-ode}.</li> <li> <strong>Stochastic sampling:</strong> Uses noisy trajectories of Reverse SDE \eqref{eq:rev-sde}.</li> </ul> <p>While deterministic sampling is often preferred for efficiency, stochastic sampling can enhance diversity by producing multiple possible reconstructions of the same noisy input.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/video/deterministic_vs_stochastic.gif"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Deterministic vs Stochastic sampling.</figcaption> </div> <h3 id="2-original-vs-modified-dynamics"><strong>2. Original vs Modified Dynamics</strong></h3> <p>Sampling with PF-ODE or the Reverse SDE ensures that each intermediate sample \(\mathbf{x}_t\) correctly follows the correct distribution \(p_t\) at every time step. We call this sampling with <strong>“original” dynamics</strong>.</p> <p>However, sticking to the original dynamics does not allow us to influence the likelihood of generated samples at \(t=0\). As we will see, extending log-density estimation to modified dynamics, where we alter the drift or diffusion terms of the sampling process, enables precise control over \(\log ⁡p_0(\mathbf{x}_0)\). This is crucial for manipulating image characteristics such as detail and sharpness.</p> <p>We define:</p> <ul> <li> <strong>Original Dynamics</strong>: The sampling process follows either the PF-ODE or Reverse SDE exactly, using the learned score function without modification.</li> <li> <strong>Modified Dynamics</strong>: The sampling process deviates from these standard dynamics by altering the drift or diffusion terms.</li> </ul> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/video/original_vs_modified_dynamics.gif"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Left: The "original" PF-ODE dynamics, where the trajectory follows the vector field. Right: an example of "modified" dynamics, where a trajectory visibly crosses the vector field to bias samples toward higher-likelihood regions.</figcaption> </div> <p>Previously, \(d \log p_t(\mathbf{x}_t)\) was only known for <strong>deterministic sampling under original dynamics</strong>—i.e., along PF-ODE trajectories <d-cite key="chen2018neural"></d-cite>.</p> <p>In <d-cite key="karczewski2025diffusion"></d-cite>, we provide formulas <d-footnote> The same two formulas were simultaneously and independently discovered by <d-cite key="skreta2025the"></d-cite></d-footnote> for \(d \log p_t(\mathbf{x}_t)\) for:</p> <ol> <li> <strong>Stochastic Sampling with Original Dynamics:</strong> We derive how log-density evolves along stochastic Reverse SDE trajectories. <d-footnote> Interestingly, we prove that, in contrast to the deterministic case, replacing the true score function \( \nabla \log p_t(\mathbf{x}) \) with an estimate \( \mathbf{s}_\theta (\mathbf{x}, t) \) makes the log-density estimation biased. This bias is given by the estimation error of the score function.</d-footnote> </li> <li> <strong>Deterministic Sampling with Modified Dynamics:</strong> We show how log-density can be estimated not just for PF-ODE trajectories but for any deterministic trajectory.</li> </ol> <p>Finally, in <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we generalize this further to:</p> <ol> <li> <strong>Stochastic Sampling with Modified Dynamics:</strong> We prove that \(d \log p_t(\mathbf{x}_t)\) can be estimated for <strong>any stochastic trajectory</strong>, not just those following the Reverse SDE. This establishes a completely general framework for log-density estimation under any sampling procedure.</li> </ol> <p>The following table summarizes these advancements:</p> <table> <thead> <tr> <th>Sampling Mode</th> <th>Original Dynamics (PF-ODE / Reverse SDE)</th> <th>Any Dynamics (Modified ODE / SDE)</th> </tr> </thead> <tbody> <tr> <td><strong>Deterministic</strong></td> <td>Prior work <d-cite key="chen2018neural"></d-cite> </td> <td>Ours <d-cite key="karczewski2025diffusion"></d-cite> </td> </tr> <tr> <td><strong>Stochastic</strong></td> <td>Ours <d-cite key="karczewski2025diffusion"></d-cite> </td> <td>Ours <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> </td> </tr> </tbody> </table> <p>Since stochastic trajectories generalize deterministic ones, our final result in <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> provides the most general formula for \(d \log p_t(\mathbf{x}_t)\) along <strong>any sampling trajectory</strong>, given by:</p> \[\begin{equation} d \mathbf{x}_t = \mathbf{u}_t(\mathbf{x}_t)dt + G_t(\mathbf{x}_t)d\overline{W}_t. \end{equation}\] <p>Note that</p> <ul> <li>For \(\mathbf{u}_t(\mathbf{x})=f(t)\mathbf{x} - g^2(t)\nabla \log p_t(\mathbf{x})\) and \(G_t(\mathbf{x}) = g(t)I\), we get the Reverse SDE \eqref{eq:rev-sde},</li> <li>For \(\mathbf{u}_t(\mathbf{x})=f(t)\mathbf{x} - \frac{1}{2}g^2(t)\nabla \log p_t(\mathbf{x})\) and \(G_t(\mathbf{x})=\mathbf{0}\), we get the PF-ODE \eqref{eq:pf-ode}</li> <li>For \(\mathbf{u}_t(\mathbf{x})=f(t)\mathbf{x} - \frac{1}{2}g^2(t) \eta \nabla \log p_t(\mathbf{x})\) and \(G_t(\mathbf{x})=\mathbf{0}\), we get a new ODE, which is biased towards higher or lower values of log-density, depending on the value of \(\eta\).</li> </ul> <p>In the next section, we show how this ability to estimate log-density under any dynamics enables us to <strong>actively control it</strong>, allowing for precise manipulation of image detail in diffusion models.</p> <h2 id="how-to-control-log-density">How to Control Log-Density?</h2> <p>The simplest approach to controlling log-density is to manipulate the latent code. Note that the PF-ODE defines the <em>solution map</em> \(\mathbf{x}_T \mapsto \mathbf{x}_0(\mathbf{x_T})\). One can thus define the objective as a function of tha latent code:</p> \[\mathcal{J}(\mathbf{x}_T) = \log p_0 (\mathbf{x}_0(\mathbf{x}_T)),\] <p>which can be directly optimized by differentiating through the ODE solver <d-cite key="ben2024d"></d-cite>. However, this procedure is significantly more expensive than regular sampling, and does not extend easily to stochastic sampling, because in stochastic sampling, the starting point \(\mathbf{x}_T\) carries very little information about where we end up \(\mathbf{x}_0\).</p> <p>We will discuss how precise density control can be achieved without extra cost by modifying the sampling dynamics, both deterministic and stochastic.</p> <h3 id="density-guidance-a-principled-approach-to-controlling-log-density">Density Guidance: A Principled Approach to Controlling Log-Density</h3> <p>In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we propose <strong>Density Guidance</strong>, a precise way to guide \(d\log p_t(\mathbf{x}_t)\) during sampling without any extra cost. We start from a general flow model</p> \[d\mathbf{x}_t=\mathbf{u}_t(\mathbf{x}_t)dt,\] <p>which we want to modify to enforce</p> \[\begin{equation}\label{eq:logp-b} d \log p_t(\mathbf{x}_t) = b_t(\mathbf{x}_t)dt \end{equation}\] <p>for a user-defined function \(b_t\). We show that the solution that satisfies this and diverges from the original drift the least is given by</p> \[\begin{equation} \tilde{\mathbf{u}}_t(\mathbf{x})=\mathbf{u}_t(\mathbf{x}) + \underbrace{\frac{\operatorname{div}\mathbf{u}_t(\mathbf{x}) + b_t(\mathbf{x})}{\|\nabla \log p_t(\mathbf{x})\|^2}\nabla \log p_t(\mathbf{x})}_{\text{log-density correction}}. \end{equation}\] <p>Note that when \(b_t(\mathbf{x}) = -\operatorname{div}\mathbf{u}_t(\mathbf{x})\), we recover the original dynamics \(\tilde{\mathbf{u}}_t = \mathbf{u}_t\). For \($b_t(\mathbf{x}) &lt; -\operatorname{div}\mathbf{u}_t(\mathbf{x})\), we get a model biased towards higher values of likelihood. In practice, this formula is most relevant to diffusion models because we already have (an approximation of) \(\nabla \log p_t(\mathbf{x})\). This is why in the following sections we assume the diffusion model with \(\mathbf{u}_t\) given by \eqref{eq:pf-ode}. The same framework can be used for any continuous-time flow model, provided the score is known.</p> <p><strong>How to choose</strong> \(b_t\)?</p> <p>While density guidance theoretically allows arbitrary changes to log-density, practical constraints must be considered. Log-density changes that are too large or too small can lead to samples falling outside the typical regions of the data distribution. We show that a carefully chosen \(b_t\) allows control of the log-density with no extra cost, keeps the samples in the typical region, and yields a very simple updated ODE:</p> \[\begin{equation}\label{eq:dgs} \mathbf{u}^{\text{DG-ODE}}_t(\mathbf{x}) = f(t)\mathbf{x} - \frac{1}{2}g^2(t)\eta_t(\mathbf{x})\nabla \log p_t(\mathbf{x}). \end{equation}\] <p>Note that \eqref{eq:dgs} is simply the PF-ODE \eqref{eq:pf-ode} with a rescaled score function <d-footnote> Interestingly, <d-cite key="karras2024guiding"></d-cite> explore scaling up the score function in the pursuit of targeting high-density regions and find resulting images lacking detail. We show that scaling the score function as in \eqref{eq:quantile-score-scaling} enables both controlling the amount of detail in both directions, but the scaling needs to be adaptive both in \(t\) and \( \mathbf{x} \) </d-footnote> by</p> \[\begin{equation}\label{eq:quantile-score-scaling} \eta_t(\mathbf{x})=1 + \frac{\sqrt{2D}\Phi^{-1}(q)}{\| \sigma_t \nabla \log p_t(\mathbf{x}) \|^2}, \end{equation}\] <p>where \(\Phi^{-1}\) is the quantile function of the standard normal distribution and \(q\) is a hyperparameter, which increases \(\log p_0(\mathbf{x}_0)\) for \(q&gt;0.5\) and decreases for \(q&lt;0.5\).</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/deterministic-steering.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Density guidance enables control over image detail. Samples generated with the pretrained EDM2 model <d-cite key="karras2024analyzing"></d-cite> using \eqref{eq:quantile-score-scaling} with different values of \(q\) </figcaption> </div> <p><strong>Take-home:</strong> <em>Density Guidance modifies the PF-ODE by rescaling the score, achieving fine-grained control of log-density over the entire sampling trajectory.</em></p> <h3 id="stochastic-sampling-with-density-guidance">Stochastic Sampling with Density Guidance</h3> <p>So far, we’ve discussed controlling log-density in deterministic settings. However, stochastic sampling introduces additional challenges and opportunities. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we show that, somewhat surprisingly, we can achieve the desired <strong>smooth</strong> evolution of log-density \(d \log p_t(\mathbf{x}_t)=b_t(\mathbf{x}_t)dt\) even for <strong>stochastic</strong> trajectories given by:<d-footnote> Technically, projecting out the score direction also introduces a small extra drift term. Empirically, this term is negligible, so we omit it in experiments. See <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> for details. </d-footnote></p> \[\begin{equation}\label{eq:stochastic-steering} d \mathbf{x}_t =\mathbf{u}_t^{\text{DG-SDE}}(\mathbf{x}_t)dt + \varphi(t)P_t(\mathbf{x}_t)d\overline{W}_t, \end{equation}\] <p>where</p> \[\begin{equation}\label{eq:stochastic-guidance-general} \mathbf{u}^{\text{DG-SDE}}_t(\mathbf{x}) = \mathbf{u}^{\text{DG-ODE}}_t(\mathbf{x})+ \underbrace{\frac{1}{2}\varphi^2(t)\frac{\Delta \log p_t(\mathbf{x})}{\| \nabla \log p_t(\mathbf{x}) \|^2}\nabla \log p_t(\mathbf{x})}_{\text{correction for added stochasticity}} \end{equation}\] <p>with \(\mathbf{u}^{\text{DG-ODE}}\) defined in \eqref{eq:dgs} and</p> \[\begin{equation} P_t(\mathbf{x}) = I - \left(\frac{\nabla \log p_t(\mathbf{x})}{\| \nabla \log p_t(\mathbf{x}) \|}\right) \hspace{-1mm} \left(\frac{\nabla \log p_t(\mathbf{x})}{\| \nabla \log p_t(\mathbf{x}) \|}\right)^T. \end{equation}\] <p>Let’s unpack this. We can add noise to the Density Guidance trajectory, but to maintain the desired evolution of log-density, we have to:</p> <ul> <li>Project the Wiener increment \(d \overline{W}_t\) with \(P_t\) onto the subspace orthogonal to the score;</li> <li>Correct the drift for the added stochasticity. To estimate \(\Delta \log p_t(\mathbf{x})=\operatorname{div} \nabla \log p_t(\mathbf{x})\), we use the Hutchinson trick <d-cite key="hutchinson1989stochastic,grathwohl2018ffjord"></d-cite> </li> </ul> <p>In practice, we set \(\varphi(t) = \widetilde{\varphi}(t)g(t)\), where \(\widetilde{\varphi}\) specifies the amount of noise relative to \(g\), which is the diffusion coefficient of \eqref{eq:rev-sde}. This simplifies \eqref{eq:stochastic-guidance-general} to</p> \[\begin{equation} \mathbf{u}^{\text{DG-SDE}}_t(\mathbf{x})=f(t)\mathbf{x} - \frac{1}{2}g^2(t)\left(\eta_t(\mathbf{x})-\widetilde{\varphi}^2(t)\frac{\Delta \log p_t(\mathbf{x})}{\| \nabla \log p_t(\mathbf{x}) \|^2}\right)\nabla \log p_t(\mathbf{x}), \end{equation}\] <p>which again boils down to the PF-ODE \eqref{eq:pf-ode} with an appropriately rescaled score function.</p> <p>This is particularly useful for balancing detail and variability in generated samples. For example:</p> <ul> <li>Adding noise early (\(\widetilde{\varphi}(t) \neq 0\) for large \(t\)) in the sampling process introduces variation in high-level features like shapes.</li> <li>Adding noise later (\(\widetilde{\varphi}(t) \neq 0\) for small \(t\)) affects only low-level details like texture.</li> </ul> <p>Our method ensures that the log-density evolution remains consistent with the deterministic case, allowing precise control while injecting controlled randomness.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/stochastic-steering.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Stochastic Density guidance allows for noise injection without sacrificing control over image detail. Samples generated with the pretrained EDM2 model <d-cite key="karras2024analyzing"></d-cite> using \eqref{eq:stochastic-steering} with different values of \(q\) and noisy injected at different stages of sampling. </figcaption> </div> <p><strong>Take-home:</strong> <em>Stochastic Density Guidance = same rescaled score approach, plus a projected noise term that preserves the intended log-density schedule.</em></p> <hr> <p>An interesting observation <d-cite key="song2021scorebased"></d-cite> is that simply rescaling the latent code (e.g., scaling the noise at the start of the sampling process) changes the amount of detail in the generated image. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we provide a theoretical explanation for this phenomenon using a concept we call Score Alignment, which directly ties the scaling of the latent code to changes in log-density.</p> <h3 id="score-alignment">Score Alignment</h3> <p>Score alignment measures the angle between:</p> <ul> <li>\(\nabla \log p_T(\mathbf{x}_T)\) (score of noise distribution) pushed forward via the flow of PF-ODE \eqref{eq:pf-ode} to \(t = 0\), and</li> <li>\(\nabla \log p_0(\mathbf{x}_0)\) (score of data distribution).</li> </ul> <p>Let’s clarify what we mean by the <a href="https://en.wikipedia.org/wiki/Pushforward_(differential)" rel="external nofollow noopener" target="_blank">“push forward”</a>. Suppose we have a curve \(\gamma\) passing through \(\mathbf{x}_T\), with velocity \(\nabla \log p_T(\mathbf{x}_T)\), i.e. \(\gamma(0)=\mathbf{x}_T\) and \(\gamma'(0)=\nabla \log p_T(\mathbf{x}_T)\).</p> <p>The pushforward of \(\nabla \log p_T(\mathbf{x}_T)\) via a map \(F\) is simply the tangent vector of the mapped curve, that is:</p> \[\frac{d}{ds} F(\gamma(s)) \bigg\rvert_{s=0}.\] <p>In our case, \(F\) is the solution map of the PF-ODE, which maps an initial noise sample \(\mathbf{x}_T\) to a final generated sample \(\mathbf{x}_0\) by solving \eqref{eq:pf-ode}. Thus, the pushforward describes how tangent vectors evolve under the PF-ODE.</p> <p>If the angle is always acute (less than \(90^{\circ}\)), scaling the latent code at \(t = T\) changes \(\log p_0(\mathbf{x}_0)\) in a monotonic way, explaining the relationship between scaling and image detail.<d-footnote> If the angle is always obtuse (more than \(90^{\circ}\)), scaling has the reverse effect, i.e. increasing \( \log p_T(\mathbf{x}_T) \) decreases \( \log p_0(\mathbf{x}_0) \) </d-footnote> Remarkably, we show that this <strong>alignment can be measured without knowing the score function</strong>, see <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite> for the proof and implementation.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/density-guidance/sa_vis.jpg"> <figcaption class="figcaption" style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Score Alignment is a condition that guarantees monotonic impact of scaling the latent code on the log-density of the decoded sample. It is tractable to verify in practice, even without knowing the score function. Empirically we verify that it almost always holds for diffusion models on image data. </figcaption> </div> <p><strong>Take-home:</strong> <em>If SA holds, simply rescaling the latent noise \(\mathbf{x}_T\) is a quick way to increase or decrease the final log-density (and thus control image detail).</em></p> <h2 id="conclusion">Conclusion</h2> <p>Log-density is a crucial concept in understanding and controlling diffusion models. It measures the level of detail in generated images rather than determining in-distribution likelihood. In <d-cite key="karczewski2025diffusion"></d-cite>, we explore the curious behavior of high-density regions in diffusion models, revealing unexpected patterns and proposing new ways to measure log-density across sampling methods. In <d-cite key="karczewski2025devildetailsdensityguidance"></d-cite>, we go further, introducing <strong>Density Guidance</strong> and demonstrating how to precisely control log-density in both deterministic and stochastic settings.</p> <p>These findings not only advance our theoretical understanding of diffusion models but also open up practical avenues for generating images with fine-grained control over detail and variability.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blogs.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Rafał Karczewski. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?c8cdae9495e998e46b0dddc10276be62"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>