---
---

@string{aps = {American Physical Society,}}

@article{karczewski2025devildetailsdensityguidance,
      title={Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models}, 
      author={Rafał Karczewski and Markus Heinonen and Vikas Garg},
      year={2025},
      journal={arXiv},
      url={https://arxiv.org/abs/2502.05807},
      pdf={https://arxiv.org/pdf/2502.05807},
      abstract={Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality images by mapping noise to a data distribution. However, recent findings suggest that image likelihood does not align with perceptual quality: high-likelihood samples tend to be smooth, while lower-likelihood ones are more detailed. Controlling sample density is thus crucial for balancing realism and detail. In this paper, we analyze an existing technique, Prior Guidance, which scales the latent code to influence image detail. We introduce score alignment, a condition that explains why this method works and show that it can be tractably checked for any continuous normalizing flow model. We then propose Density Guidance, a principled modification of the generative ODE that enables exact log-density control during sampling. Finally, we extend Density Guidance to stochastic sampling, ensuring precise log-density control while allowing controlled variation in structure or fine details. Our experiments demonstrate that these techniques provide fine-grained control over image detail without compromising sample quality. },
      selected=true
}

@inproceedings{
karczewski2025diffusion,
title={Diffusion Models as Cartoonists! The Curious Case of High Density Regions},
author={Rafał Karczewski and Markus Heinonen and Vikas Garg},
booktitle={ICLR},
year={2025},
url={https://openreview.net/forum?id=RiS2cxpENN},
abstract={We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-probability sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost. },
pdf={https://arxiv.org/pdf/2411.01293},
selected=true
}

@inproceedings{
karczewski2025what,
title={What Ails Generative Structure-based Drug Design: Expressivity is Too Little or Too Much?},
author={Rafał Karczewski and Samuel Kaski and Markus Heinonen and Vikas K Garg},
booktitle={AISTATS},
year={2025},
honor={Oral Presentation [Top 2\%]},
url={https://openreview.net/forum?id=GHyBMTpiJg},
pdf={https://arxiv.org/pdf/2408.06050},
code={https://github.com/rafalkarczewski/SimpleSBDD},
abstract={Several generative models with elaborate training and sampling procedures have been proposed recently to accelerate structure-based drug design (SBDD); however, perplexingly, their empirical performance turns out to be suboptimal. We seek to better understand this phenomenon from both theoretical and empirical perspectives. Since most of these models apply graph neural networks (GNNs), one may suspect that they inherit the representational limitations of GNNs. We analyze this aspect, establishing the first such results for protein-ligand complexes. A plausible counterview may attribute the underperformance of these models to their excessive parameterizations, inducing expressivity at the expense of generalization. We also investigate this possibility with a simple metric-aware approach that learns an economical surrogate for affinity to infer an unlabelled molecular graph and optimizes for labels conditioned on this graph and molecular properties. The resulting model achieves state-of-the-art results using 100x fewer trainable parameters and affords up to 1000x speedup. Collectively, our findings underscore the need to reassess and redirect the existing paradigm and efforts for SBDD.},
selected=true
}

@inproceedings{
karczewski2024on,
title={On the Generalization of Equivariant Graph Neural Networks},
author={Rafał Karczewski and Amauri H Souza and Vikas Garg},
booktitle={ICML},
year={2024},
url={https://openreview.net/forum?id=Yqj3DzIC79},
pdf={https://openreview.net/pdf?id=Yqj3DzIC79},
code={https://github.com/Aalto-QuML/GeneralizationEGNNs},
abstract={E(n)-Equivariant Graph Neural Networks (EGNNs) are among the most widely used and successful models for representation learning on geometric graphs (e.g., 3D molecules). However, while the expressivity of EGNNs has been explored in terms of geometric variants of the Weisfeiler-Leman isomorphism test, characterizing their generalization capability remains open. In this work, we establish the first generalization bound for EGNNs. Our bound depicts a dependence on the weighted sum of logarithms of the spectral norms of the weight matrices (EGNN parameters). In addition, our main result reveals interesting novel insights: i) the spectral norms of the initial layers may impact generalization more than the final ones; ii) epsilon-normalization is beneficial to generalization --- confirming prior empirical evidence. We leverage these insights to introduce a spectral norm regularizer tailored to EGNNs. Experiments on real-world datasets substantiate our analysis, demonstrating a high correlation between theoretical and empirical generalization gaps and the effectiveness of the proposed regularization scheme.}
}

@article{mozejko2018inhibited,
  title={Inhibited softmax for uncertainty estimation in neural networks},
  author={Mo{\.z}ejko, Marcin and Susik, Mateusz and Karczewski, Rafał},
  journal={arXiv},
  abstract={We present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output. We extend softmax layer with an additional constant input. The corresponding additional output is able to represent the uncertainty of the network. The proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets. We show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains. },
  pdf={https://arxiv.org/pdf/1810.01861},
  year={2018}
}

@article{Karczewski04072017,
author = {Rafał Karczewski and Jacek Wesołowski},
title = {Linearity of regression for weak records, revisited},
journal = {Statistics},
volume = {51},
number = {4},
pages = {878--887},
year = {2017},
publisher = {Taylor \& Francis},
doi = {10.1080/02331888.2017.1301940},
URL = {https://doi.org/10.1080/02331888.2017.1301940},
eprint = {https://doi.org/10.1080/02331888.2017.1301940}
}